{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pyproj\n",
    "\n",
    "geod = pyproj.Geod(ellps='WGS84')\n",
    "from multiprocessing.pool import Pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings (file names and directories)\n",
    "\n",
    "LABELS_FILE = 'labels.txt'\n",
    "MAIN_FOLDER = './data/'\n",
    "TRAJ_FOLDER = 'Trajectory/'\n",
    "OUTPUT_FOLDER = './data_preprocessed3/'\n",
    "POOLSIZE = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance between 2 points\n",
    "def calculate_distance(long1, lat1, long2, lat2):\n",
    "    if lat1 == lat2 and long1 == long2:\n",
    "        return 0\n",
    "    if False in np.isfinite([long1, long2, lat1, lat2]):\n",
    "        return np.nan\n",
    "    if lat1 < -90 or lat1 > 90 or lat2 < -90 or lat2 > 90:\n",
    "        #raise ValueError('The range of latitudes seems to be invalid.')\n",
    "        return np.nan\n",
    "    if long1 < -180 or long1 > 180 or long2 < -180 or long2 > 180:\n",
    "        return np.nan\n",
    "        #raise ValueError('The ranя ge of longitudes seems to be invalid.')\n",
    "    angle1,angle2,distance = geod.inv(long1, lat1, long2, lat2)\n",
    "    return distance\n",
    "\n",
    "# Get velocity distance/timedelta\n",
    "def calculate_velocity(distance, timedelta):\n",
    "    if timedelta.total_seconds() == 0: return np.nan\n",
    "    return distance / timedelta.total_seconds()\n",
    "\n",
    "# Get acceleration diff(velocity)/timedelta\n",
    "def calculate_acceleration(velocity, velocity_next_position, timedelta):\n",
    "    delta_v = velocity_next_position - velocity\n",
    "    if timedelta.total_seconds() == 0: return np.nan\n",
    "    return delta_v / timedelta.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_trajectory = ['lat', 'long', 'null', 'altitude','timestamp_float', 'date', 'time']\n",
    "\n",
    "# Convert str datetime to datetime format\n",
    "def to_datetime(string):\n",
    "    return dt.datetime.strptime(string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Load labels in the folder\n",
    "def load_labels_df(filename):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df['start_time'] = df['Start Time'].apply(lambda x: dt.datetime.strptime(x, '%Y/%m/%d %H:%M:%S'))\n",
    "    df['end_time'] = df['End Time'].apply(lambda x: dt.datetime.strptime(x, '%Y/%m/%d %H:%M:%S'))\n",
    "    df['labels'] = df['Transportation Mode']\n",
    "    df = df.drop(['End Time', 'Start Time', 'Transportation Mode'], axis=1)\n",
    "    return df\n",
    "\n",
    "# Load trajectory\n",
    "def load_trajectory_df(full_filename):\n",
    "    subfolder = full_filename.split('/')[-3]\n",
    "    trajectory_id = full_filename.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    df = pd.read_csv(full_filename, skiprows = 6, header = None, names = headers_trajectory)\n",
    "   \n",
    "    df['datetime'] = df.apply(lambda z: to_datetime(z.date + ' ' + z.time), axis=1)\n",
    "    df['datetime_next_position'] = df['datetime'].shift(-1)\n",
    "    df['timedelta'] = df.apply(lambda z: z.datetime_next_position - z.datetime, axis=1)\n",
    "    df = df.drop(['datetime_next_position'], axis=1)\n",
    "    df = df.drop(['null', 'timestamp_float', 'date', 'time'], axis=1)\n",
    "    \n",
    "    df['long_next_position'] = df['long'].shift(-1)\n",
    "    df['lat_next_position'] = df['lat'].shift(-1)\n",
    "    df['distance'] = df.apply(lambda z: calculate_distance(z.long, z.lat, z.long_next_position, z.lat_next_position), axis=1)\n",
    "    df = df.drop(['long_next_position', 'lat_next_position'], axis=1)\n",
    "    \n",
    "    df['velocity'] = df.apply(lambda z: calculate_velocity(z.distance, z.timedelta), axis=1)\n",
    "    df['velocity_next_position'] = df['velocity'].shift(-1)\n",
    "    df['acceleration'] = df.apply(lambda z: calculate_acceleration(z.velocity, z.velocity_next_position, z.timedelta), axis=1)\n",
    "    df = df.drop(['velocity_next_position'], axis=1)\n",
    "    \n",
    "    df['trajectory_id'] = trajectory_id\n",
    "    df['subfolder'] = subfolder\n",
    "    df['labels'] = ''\n",
    "    calculate_agg_features(df)\n",
    "    return df\n",
    "\n",
    "#This method calculates the aggregated feature and \n",
    "#saves them in the original df\n",
    "def calculate_agg_features(df):\n",
    "    v_ave = np.nanmean(df['velocity'].values)\n",
    "    v_med = np.nanmedian(df['velocity'].values)\n",
    "    v_max = np.nanmax(df['velocity'].values)\n",
    "    v_std = np.nanstd(df['velocity'].values)\n",
    "    \n",
    "    a_ave = np.nanmean(df['acceleration'].values)\n",
    "    a_med = np.nanmedian(df['acceleration'].values)\n",
    "    a_max = np.nanmax(df['acceleration'].values)\n",
    "    a_std = np.nanstd(df['acceleration'].values)\n",
    "   \n",
    "    df.loc[:, 'v_ave'] = v_ave\n",
    "    df.loc[:, 'v_med'] = v_med\n",
    "    df.loc[:, 'v_max'] = v_max\n",
    "    df.loc[:, 'v_std'] = v_std\n",
    "    \n",
    "    df.loc[:, 'a_ave'] = a_ave\n",
    "    df.loc[:, 'a_med'] = a_med\n",
    "    df.loc[:, 'a_max'] = a_max\n",
    "    df.loc[:, 'a_std'] = a_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 156\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "directories = os.listdir(MAIN_FOLDER)\n",
    "\n",
    "# Garbage collector used to collect garbage \n",
    "# becouse OS handles it slow\n",
    "import gc\n",
    "\n",
    "# Go to the dirs and try load it.\n",
    "# Save in the preprocessed folder\n",
    "for subfolder in directories:\n",
    "    list_df_traj = []\n",
    "    subfolder_ = MAIN_FOLDER + subfolder + '/'\n",
    "    traj_folder = MAIN_FOLDER + subfolder + '/' + TRAJ_FOLDER\n",
    "    traj_files = os.listdir(traj_folder)\n",
    "    \n",
    "    traj_files_full_path = [traj_folder + traj_file for traj_file in traj_files]\n",
    "    print(subfolder, len(traj_files_full_path))\n",
    "    \n",
    "    for i, file in enumerate(traj_files_full_path):\n",
    "        list_df_traj.append(load_trajectory_df(file))\n",
    "    \n",
    "    # Get list of trajectories and append to the global df_traj_all\n",
    "    # then clean it\n",
    "    df_traj_all = pd.concat(list_df_traj)\n",
    "    list_df_traj = []\n",
    "    \n",
    "    # If there label file in the folder do preprocessing\n",
    "    if LABELS_FILE in os.listdir(subfolder_):\n",
    "        filename = subfolder_ + LABELS_FILE\n",
    "        df_labels = load_labels_df(filename)\n",
    "        for idx in df_labels.index.values:\n",
    "            st = df_labels.ix[idx]['start_time']\n",
    "            et = df_labels.ix[idx]['end_time']\n",
    "            labels = df_labels.ix[idx]['labels']\n",
    "            if labels:\n",
    "                df_traj_all.loc[(df_traj_all['datetime'] >= st) & \n",
    "                                (df_traj_all['datetime'] <= et), 'labels'] = labels\n",
    "    \n",
    "    # Save file with labels and aggegated features\n",
    "    output_filename = OUTPUT_FOLDER + subfolder + '.csv'\n",
    "    df_traj_all.to_csv(output_filename)\n",
    "    \n",
    "    # Delete file and try to erise memory with gc(garbage collector) \n",
    "    del df_traj_all\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "### 1) All GPS and labels data converted to the one CSV file.\n",
    "\n",
    "### 2) Created features like velocity, acceleration, data preprocessing.\n",
    "\n",
    "### 3) Created statistics like max, std, mean, median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure before             |  Data sctructure after\n",
    ":-------------------------:|:-------------------------:\n",
    "![title](jupyter_images/0. Data Preparation/raw_data_folder.jpg)  |  ![title](jupyter_images/0. Data Preparation/preprocessed_data.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
